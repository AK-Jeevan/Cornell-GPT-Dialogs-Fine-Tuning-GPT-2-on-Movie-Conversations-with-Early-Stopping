# CornellGPTâ€‘Dialogs ğŸ¬ğŸ¤–
Fineâ€‘tuning GPTâ€‘2 on the **Cornell Movie Dialogs dataset** with Hugging Face Transformers.  
This project demonstrates conversational AI training with **early stopping** to prevent overfitting.

---

## âœ¨ Features
- Load and preprocess the Cornell Movie Dialogs dataset
- Tokenize text using GPTâ€‘2 tokenizer
- Fineâ€‘tune GPTâ€‘2 with Hugging Face `Trainer`
- Apply **EarlyStoppingCallback** for efficient training
- Generate sample dialogue responses

---

## ğŸ“‚ Dataset
We use the [Cornell Movie Dialogs dataset](https://huggingface.co/datasets/cornell_movie_dialog), which contains:
- 220,000+ conversational exchanges
- Extracted from movie scripts
- Ideal for chatbot training and dialogue modeling

---

## âš™ï¸ Installation

### Clone the repo
git clone https://github.com/your-username/CornellGPT-Dialogs.git
cd CornellGPT-Dialogs

### Install dependencies
pip install -r requirements.txt

#### Dependencies:

transformers

datasets

torch

## ğŸ“Š Results
Faster convergence with early stopping

Reduced overfitting on small dialogue dataset

Generates conversational responses after fineâ€‘tuning

## ğŸ“Œ Future Work
Add custom metrics (e.g., perplexity) for monitoring

Experiment with larger context windows

Fineâ€‘tune on multiâ€‘domain dialogue datasets

## ğŸ“œ License
MIT License. Free to use and modify. 
